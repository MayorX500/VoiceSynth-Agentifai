{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5Yry22YPDRp",
    "outputId": "9083ff69-d98e-4c5d-a888-966c91e2b312"
   },
   "outputs": [],
   "source": [
    "DATASET_URL = \"http://mayorx.xyz/Media/agentifai/podcast_dataset.zip\"\n",
    "DATASET_PATH = \"podcast_dataset.zip\"\n",
    "DATASET_DIR = \"dataset/\"\n",
    "DATASET_NAME = \"bitalk_podcast\"\n",
    "DATASET_LANG = \"pt\"\n",
    "\n",
    "\n",
    "INPUT_VOICE_URL = \"http://mayorx.xyz/Media/agentifai/input_voice.wav\"\n",
    "INPUT_VOICE_PATH = \"input/voices/input_voice.wav\"\n",
    "INPUT_VOICE_DIR = \"input/voices/\"\n",
    "\n",
    "MODEL_LATEST_URL = \"http://mayorx.xyz/Media/agentifai/model_latest.pth\"\n",
    "MODEL_LATEST_VOCAB = \"https://huggingface.co/coqui/XTTS-v2/resolve/main/vocab.json\"\n",
    "MODEL_LATEST_MEL = \"https://huggingface.co/coqui/XTTS-v2/resolve/main/mel_stats.pth\"\n",
    "MODEL_LATEST_DVAE = \"https://huggingface.co/coqui/XTTS-v2/resolve/main/dvae.pth\"\n",
    "MODEL_LATEST_MODEL_PATH = \"run/training/XTTS_v2.0_original_model_files/model.pth\"\n",
    "MODEL_LATEST_VOCAB_PATH = \"run/training/XTTS_v2.0_original_model_files/vocab.json\"\n",
    "MODEL_LATEST_MEL_PATH = \"run/training/XTTS_v2.0_original_model_files/mel_stats.pth\"\n",
    "MODEL_LATEST_DVAE_PATH = \"run/training/XTTS_v2.0_original_model_files/dvae.pth\"\n",
    "MODEL_LATEST_DIR = \"run/training/XTTS_v2.0_original_model_files\"\n",
    "\n",
    "MODEL_BASE_URL = \"https://huggingface.co/coqui/XTTS-v2/resolve/main/model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1W0vEhnAAT0l",
    "outputId": "2a2d9225-de75-45c4-fa72-3d58f719ff0f"
   },
   "outputs": [],
   "source": [
    "# Get dataset single_speaker\n",
    "!mkdir -p $INPUT_VOICE_DIR $MODEL_LATEST_DIR\n",
    "\n",
    "# Download dataset\n",
    "!wget $DATASET_URL -O $DATASET_PATH\n",
    "!unzip $DATASET_PATH\n",
    "!rm $DATASET_PATH\n",
    "\n",
    "# Download input voice\n",
    "!wget $INPUT_VOICE_URL -O $INPUT_VOICE_PATH\n",
    "\n",
    "# Download model to FineTune\n",
    "!wget $MODEL_LATEST_URL -O $MODEL_LATEST_MODEL_PATH\n",
    "!wget $MODEL_LATEST_VOCAB -O $MODEL_LATEST_VOCAB_PATH\n",
    "!wget $MODEL_LATEST_MEL -O $MODEL_LATEST_MEL_PATH\n",
    "!wget $MODEL_LATEST_DVAE_PATH -O $MODEL_LATEST_DVAE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "3WlhZWPd-a-E",
    "outputId": "c3156dca-594f-4d7a-b42e-913033275f7f"
   },
   "outputs": [],
   "source": [
    "!apt install espeak-ng ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1DuXcIln62Gy",
    "outputId": "58223b64-7dd6-41db-a669-9eabae1eacc3"
   },
   "outputs": [],
   "source": [
    "!rm -rf coqui-ai-TTS\n",
    "!git clone https://github.com/idiap/coqui-ai-TTS\n",
    "!pip install -e coqui-ai-TTS/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldPTsKnB-2aE"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('coqui-ai-TTS')\n",
    "\n",
    "import os\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
    "from TTS.utils.manage import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEvX83hN6rm2",
    "outputId": "99c5a185-b491-4ae6-9001-d9cc628673d0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Logging parameters\n",
    "RUN_NAME = \"XTTS_FineTuned\"\n",
    "PROJECT_NAME = \"XTTS_trainer\"\n",
    "DASHBOARD_LOGGER = \"tensorboard\"\n",
    "LOGGER_URI = None\n",
    "\n",
    "# Set here the path that the checkpoints will be saved. Default: ./run/training/\n",
    "OUT_PATH = os.path.join(\"model\", \"training\")\n",
    "\n",
    "# Training Parameters\n",
    "OPTIMIZER_WD_ONLY_ON_WEIGHTS = True  # for multi-gpu training please make it False\n",
    "START_WITH_EVAL = True  # if True it will star with evaluation\n",
    "BATCH_SIZE = 8  # set here the batch size\n",
    "GRAD_ACUMM_STEPS = 48  # set here the grad accumulation steps\n",
    "# Note: we recommend that BATCH_SIZE * GRAD_ACUMM_STEPS need to be at least 252 for more efficient training. You can increase/decrease BATCH_SIZE but then set GRAD_ACUMM_STEPS accordingly.\n",
    "\n",
    "# Define here the dataset that you want to use for the fine-tuning on.\n",
    "config_dataset = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\",\n",
    "    dataset_name=DATASET_NAME,\n",
    "    path=DATASET_PATH,\n",
    "    meta_file_train=\"dataset/metadata.csv\",\n",
    "    language=DATASET_LANG,\n",
    ")\n",
    "\n",
    "# Add here the configs of the datasets\n",
    "DATASETS_CONFIG_LIST = [config_dataset]\n",
    "\n",
    "# Define the path where XTTS v2.0.1 files will be downloaded\n",
    "CHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"XTTS_v2.0_original_model_files/\")\n",
    "os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "# DVAE files\n",
    "DVAE_CHECKPOINT_LINK = MODEL_LATEST_DVAE\n",
    "MEL_NORM_LINK = MODEL_LATEST_MEL\n",
    "\n",
    "# Set the path to the downloaded files\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))\n",
    "\n",
    "# download DVAE files if needed\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\" > Downloading DVAE files!\")\n",
    "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)\n",
    "\n",
    "\n",
    "# Download XTTS v2.0 checkpoint if needed\n",
    "TOKENIZER_FILE_LINK = MODEL_LATEST_VOCAB\n",
    "\n",
    "XTTS_CHECKPOINT_LINK = MODEL_LATEST_URL\n",
    "\n",
    "# XTTS transfer learning parameters: You we need to provide the paths of XTTS model checkpoint that you want to do the fine tuning.\n",
    "TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json file\n",
    "XTTS_CHECKPOINT = MODEL_LATEST_MODEL_PATH  # model.pth file\n",
    "\n",
    "# download XTTS v2.0 files if needed\n",
    "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
    "    print(\" > Downloading XTTS v2.0 files!\")\n",
    "    ModelManager._download_model_files(\n",
    "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Training sentences generations\n",
    "SPEAKER_REFERENCE = [\n",
    "    INPUT_VOICE_PATH  # speaker reference to be used in training test sentences\n",
    "]\n",
    "LANGUAGE = config_dataset.language\n",
    "\n",
    "\n",
    "def main():\n",
    "    # init args and config\n",
    "    model_args = GPTArgs(\n",
    "        max_conditioning_length=441000,  # 20 secs\n",
    "        min_conditioning_length=22050,  # 1 secs\n",
    "        debug_loading_failures=False,\n",
    "        max_wav_length=441000,  # ~11.6 seconds\n",
    "        max_text_length=256,\n",
    "        mel_norm_file=MEL_NORM_FILE,\n",
    "        dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "        xtts_checkpoint=XTTS_CHECKPOINT,  # checkpoint path of the model that you want to fine-tune\n",
    "        tokenizer_file=TOKENIZER_FILE,\n",
    "        gpt_num_audio_tokens=1026,\n",
    "        gpt_start_audio_token=1024,\n",
    "        gpt_stop_audio_token=1025,\n",
    "        gpt_use_masking_gt_prompt_approach=True,\n",
    "        gpt_use_perceiver_resampler=True,\n",
    "    )\n",
    "    # define audio config\n",
    "    audio_config = XttsAudioConfig(sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=22050)\n",
    "    # training parameters config\n",
    "    config = GPTTrainerConfig(\n",
    "        epochs=1000,\n",
    "        output_path=OUT_PATH,\n",
    "        model_args=model_args,\n",
    "        run_name=RUN_NAME,\n",
    "        project_name=PROJECT_NAME,\n",
    "        run_description=\"\"\"\n",
    "            GPT XTTS training\n",
    "            \"\"\",\n",
    "        dashboard_logger=DASHBOARD_LOGGER,\n",
    "        logger_uri=LOGGER_URI,\n",
    "        audio=audio_config,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        batch_group_size=32,\n",
    "        eval_batch_size=BATCH_SIZE,\n",
    "        num_loader_workers=8,\n",
    "        eval_split_max_size=256,\n",
    "        print_step=100,\n",
    "        plot_step=100,\n",
    "        log_model_step=1000,\n",
    "        save_step=10000,\n",
    "        save_n_checkpoints=1,\n",
    "        save_checkpoints=True,\n",
    "        # target_loss=\"loss\",\n",
    "        print_eval=False,\n",
    "        use_phonemes=True,\n",
    "        phoneme_language=DATASET_LANG,\n",
    "        phonemizer=\"espeak-ng\",\n",
    "        # Optimizer values like tortoise, pytorch implementation with modifications to not apply WD to non-weight parameters.\n",
    "        optimizer=\"radam\",\n",
    "        optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n",
    "        optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "        lr=5e-06,  # learning rate\n",
    "        lr_scheduler=\"MultiStepLR\",\n",
    "        # it was adjusted accordly for the new step scheme\n",
    "        lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "        test_sentences=[\n",
    "          {\n",
    "              \"text\": \"Olá\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Obrigado\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Como estás\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Até amanhã\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Vamos ao cinema\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Está um dia bonito hoje\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Preciso de comprar pão na padaria\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"O cão correu atrás da bola no parque\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Ela leu o livro todo numa tarde chuvosa\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"O professor explicou a lição com muita paciência\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Eles decidiram viajar pelo país durante as férias\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"O sol brilhava intensamente no céu sem nuvens\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"No inverno gostamos de beber chá quente junto à lareira\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"A cidade estava cheia de luzes e decorações de Natal\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Estudamos juntos na biblioteca para preparar o exame final\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"O concerto foi magnífico com músicos talentosos e melodias emocionantes\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Depois de uma longa caminhada descansaram à sombra de uma árvore\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Ela preparou uma surpresa especial para o aniversário do irmão\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Os cientistas descobriram uma nova espécie de planta na floresta tropical\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"A exposição de arte contemporânea atraiu muitos visitantes este ano\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Apesar da chuva decidiram continuar com o piquenique no parque\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Os atletas treinaram arduamente para se prepararem para a competição\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"A biblioteca oferece uma vasta coleção de livros e recursos digitais\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Durante a viagem aprenderam muito sobre as culturas locais e tradições\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"Com determinação e esforço alcançou todos os seus objetivos profissionais\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "          {\n",
    "              \"text\": \"No fim de semana planearam visitar a aldeia histórica e explorar as ruínas antigas\",\n",
    "              \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "              \"language\": LANGUAGE\n",
    "          },\n",
    "      ],\n",
    "    )\n",
    "\n",
    "    # init the model from config\n",
    "    model = GPTTrainer.init_from_config(config)\n",
    "\n",
    "    # load training samples\n",
    "    train_samples, eval_samples = load_tts_samples(\n",
    "        DATASETS_CONFIG_LIST,\n",
    "        eval_split=True,\n",
    "        eval_split_max_size=config.eval_split_max_size,\n",
    "        eval_split_size=config.eval_split_size,\n",
    "    )\n",
    "\n",
    "    # init the trainer and 🚀\n",
    "    trainer = Trainer(\n",
    "        TrainerArgs(\n",
    "            restore_path=None,  # xtts checkpoint is restored via xtts_checkpoint key so no need of restore it using Trainer restore_path parameter\n",
    "            skip_train_epoch=False,\n",
    "            start_with_eval=START_WITH_EVAL,\n",
    "            grad_accum_steps=GRAD_ACUMM_STEPS,\n",
    "        ),\n",
    "        config,\n",
    "        output_path=OUT_PATH,\n",
    "        model=model,\n",
    "        train_samples=train_samples,\n",
    "        eval_samples=eval_samples,\n",
    "    )\n",
    "    trainer.fit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
